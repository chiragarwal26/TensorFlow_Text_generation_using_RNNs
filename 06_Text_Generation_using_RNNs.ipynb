{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNNs Romeo Juleit Play Generator"
      ],
      "metadata": {
        "id": "fnhMvWv9-TzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDWICdHF-KF2",
        "outputId": "fe1e296e-5b01-4d6e-ba39-b64f27f411cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x # this line is not required unless you  r in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "tQPGvtdM_JIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3f9f92c-5f7e-4e4a-cd7d-b597441fc089"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can load our own text data also\n",
        "#just uncomment the below code if you want your own data\n",
        "######################################################\n",
        "######################################################\n",
        "#from google.colab import files\n",
        "#path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "Dl7H-ila_ahU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URrzkGFXAHuB",
        "outputId": "2f5599b3-59c3-4e63-96c9-403cbb9f78f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp3zZnWOASlx",
        "outputId": "998e59fc-e05b-4bec-d3b3-33b112ff9124"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ENCODING\n",
        "since the text isn't encoded\n",
        "We are going to encode each unique character as a different integer by ourself"
      ],
      "metadata": {
        "id": "I6JIUXG2AjXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40FPn0OEAaxm",
        "outputId": "aaa52de1-b286-4d96-df08-b7848d242467"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i , u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "BbR-o9bfBXSg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets have a look at how our text is encoded\n",
        "print(\"TEXT : \", text[:13])\n",
        "print(\"ENCODED : \", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "totUD-hCCFh-",
        "outputId": "89fbd9ef-9e5a-46af-cead-ade3b3049498"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT :  First Citizen\n",
            "ENCODED :  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding function,  just i n case we want to see text.\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdyXj6XoCxcI",
        "outputId": "1690cda2-ae1f-4c99-a9e5-1953d1bdd71e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING OUR MODEL\n",
        "# The tarinig example we r preparing will use a seq_length as input\n",
        "# and a seq_length as O/P where sequence is original sequence shifted one letter to the right\n",
        "# FOR EXAMPLE : INPUT: Hell || OUTPUT : ello\n",
        "seq_length = 100 # length of sequence for a training example\n",
        "example_per_epoch = len(text)//(seq_length+1)\n",
        "# Create a training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "Ffo8yOPvD3qL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can use the batch method to turn this stream of characters into batches of desired length\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "zYW4scW7F7lY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example : hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text # hell, ello\n",
        "dataset = sequences.map(split_input_target) # we use map to apply the above funnction to every entry\n"
      ],
      "metadata": {
        "id": "PgnbA6LrHnQP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgYQuNsBInXq",
        "outputId": "3d72f9b0-823d-4d69-d9a0-78562c14906f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally We can make our training batches\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) # vocab is umber of unique characters \n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "BBmh5SJ4JOWD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buliding our model\n",
        "# the reason of building our model is to get flexibility\n",
        "# as for now, our model is trained for batch_size = 64\n",
        "# as we will generalise the build_model function\n",
        "# we can change and predict for batch_size = 1 \n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size,embedding_dim,\n",
        "                                batch_input_shape=[batch_size,None]),\n",
        "      tf.keras.layers.LSTM(rnn_units,\n",
        "                           return_sequences=True,\n",
        "                           stateful=True,\n",
        "                           recurrent_initializer='glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7oyMX64JY7l",
        "outputId": "a9b790c3-b5dd-48bc-e97a-72a250049d48"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATING A LOSS FUNCTION\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "    example_batch_predictions = model(input_example_batch) #ask our model for a prediction based on first batch of our training dataset \n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") #print out the output shape\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oJ0N-FzK651",
        "outputId": "6193f4e5-edc3-4050-e637-70e3d9c5f313"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that the prediction is an array of 64 arrays\n",
        "# one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "id": "lSRPvi50NMZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e61e01-7391-4d0f-8131-8deb49c7adf8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 4.84519359e-03 -5.20324567e-04  6.50518853e-03 ... -2.11109454e-03\n",
            "   -1.22855022e-03 -4.75838664e-04]\n",
            "  [ 5.55965235e-04  1.08737824e-03  9.14832950e-03 ... -5.33894030e-03\n",
            "   -4.00131848e-03  4.67943959e-03]\n",
            "  [ 2.37424695e-03  8.18407070e-03  8.83196760e-03 ... -2.79903295e-03\n",
            "   -1.72661385e-03  2.93124001e-03]\n",
            "  ...\n",
            "  [ 2.91737751e-03 -6.84425700e-04  9.18210950e-03 ...  6.68871915e-03\n",
            "   -7.73472060e-03  6.57017156e-03]\n",
            "  [-4.45492519e-03 -1.70314056e-03  1.15542868e-02 ...  5.35736699e-03\n",
            "   -1.24955345e-02  5.32854348e-03]\n",
            "  [-3.56473704e-03 -3.07771401e-03  4.80464799e-03 ...  5.82648162e-03\n",
            "   -6.90243579e-03  6.58657914e-03]]\n",
            "\n",
            " [[-6.44791406e-03 -1.48787710e-03  5.71271218e-03 ... -4.61409945e-04\n",
            "   -5.27471490e-03 -8.94126075e-04]\n",
            "  [-8.59866105e-03  3.28411628e-03  8.99816304e-03 ...  2.93477159e-03\n",
            "   -4.91230516e-03 -1.50405220e-03]\n",
            "  [-9.14547686e-03  2.73040263e-03  9.91271436e-03 ... -1.24208513e-03\n",
            "   -6.53607352e-03  3.72680649e-03]\n",
            "  ...\n",
            "  [ 2.64815846e-03  9.93584841e-03  3.99939716e-03 ...  4.19097114e-03\n",
            "   -1.22999633e-03  5.27815241e-03]\n",
            "  [ 5.11723664e-03  5.60829649e-03  7.77746900e-04 ... -2.30502035e-03\n",
            "   -1.95182092e-03  2.42818589e-03]\n",
            "  [ 4.75368346e-04  4.04126896e-03  1.77787058e-03 ... -4.73837322e-03\n",
            "   -1.84880616e-03  4.84779850e-03]]\n",
            "\n",
            " [[ 1.56556070e-03  2.79792002e-05  4.69732098e-03 ...  2.75922520e-03\n",
            "    2.91973772e-03  5.31548262e-03]\n",
            "  [ 7.02990219e-04 -2.14841473e-03  1.98211055e-03 ...  1.64655130e-03\n",
            "    5.81620075e-03  5.04716858e-03]\n",
            "  [-1.29605597e-03 -1.95827521e-03  6.62186276e-03 ... -3.44683882e-03\n",
            "    2.50584097e-03  8.11683945e-03]\n",
            "  ...\n",
            "  [-1.57697848e-03 -3.88951087e-03  2.21589534e-03 ... -3.35908635e-03\n",
            "   -1.45130921e-02  2.95409700e-03]\n",
            "  [-5.85489161e-03 -3.55529087e-03  7.83151109e-03 ... -1.46767823e-03\n",
            "   -1.69263948e-02  1.25143887e-03]\n",
            "  [ 2.44408008e-03 -2.01932760e-03  2.91127618e-03 ...  2.66695675e-03\n",
            "   -5.92411123e-03  4.52218205e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.44791406e-03 -1.48787710e-03  5.71271218e-03 ... -4.61409945e-04\n",
            "   -5.27471490e-03 -8.94126075e-04]\n",
            "  [ 1.12491613e-03 -2.32697697e-03  5.60905132e-03 ...  3.95058771e-04\n",
            "   -1.96165871e-03 -4.44097619e-04]\n",
            "  [ 3.01572261e-04  1.02288439e-04  1.92819769e-03 ...  2.90933554e-03\n",
            "   -1.46677240e-03 -1.34268531e-03]\n",
            "  ...\n",
            "  [-5.47936186e-03  2.03265180e-03  1.01024536e-02 ... -9.09818220e-04\n",
            "   -9.42990463e-03  7.79533572e-03]\n",
            "  [-1.04921684e-02  4.01741359e-04  1.41503643e-02 ... -8.77073035e-07\n",
            "   -1.28015354e-02  5.77069726e-03]\n",
            "  [-7.76173826e-03  3.08624748e-03  9.51399095e-03 ...  3.34441941e-03\n",
            "   -9.67299193e-03  3.23062949e-03]]\n",
            "\n",
            " [[-6.97452109e-04  1.90822338e-03 -1.43745937e-03 ...  1.93162868e-03\n",
            "    3.29305709e-04 -1.27468060e-03]\n",
            "  [-6.40006969e-04  5.01207029e-03  1.57321314e-03 ...  4.21302486e-03\n",
            "   -7.87716708e-04  3.87968519e-03]\n",
            "  [ 3.02066474e-05  3.32069909e-03  6.68039825e-03 ...  5.99132851e-03\n",
            "    2.27365852e-03  9.68628284e-03]\n",
            "  ...\n",
            "  [ 3.43149947e-03  2.20831507e-03  1.69780832e-02 ...  7.89710879e-03\n",
            "   -6.42665848e-03  5.16846590e-03]\n",
            "  [ 2.31564278e-03  4.77808155e-03  1.10786939e-02 ...  9.60097369e-03\n",
            "   -5.33699384e-03  3.41186766e-03]\n",
            "  [ 4.06538742e-03  1.05649736e-02  8.53603985e-03 ...  5.72042074e-03\n",
            "   -3.35531216e-03  4.37282631e-03]]\n",
            "\n",
            " [[-1.39708759e-03 -1.50011573e-03 -3.14737670e-03 ... -7.74045882e-04\n",
            "    2.94626178e-03  9.42095648e-05]\n",
            "  [-7.98775069e-03 -3.32875527e-03  2.46323389e-03 ... -1.28066004e-03\n",
            "   -4.01668064e-03 -1.43462326e-03]\n",
            "  [-5.23072481e-03  1.06417504e-03  2.54513067e-03 ...  1.70199992e-03\n",
            "   -5.11360262e-03  2.37932964e-03]\n",
            "  ...\n",
            "  [-3.62627674e-03  6.15394674e-06  1.25074005e-02 ... -3.67794605e-03\n",
            "   -7.74868717e-03 -1.99197792e-03]\n",
            "  [ 2.12654471e-04  7.44885067e-04  9.48058814e-03 ... -2.08141655e-03\n",
            "   -1.60880713e-03 -4.41015325e-03]\n",
            "  [ 1.45226973e-03  3.09735676e-03  7.41772400e-03 ...  1.26434630e-03\n",
            "   -4.54723556e-03  4.15861607e-04]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice that this is a 2d array of length 100, where\n",
        "# each interior array is prediction for the next character\n",
        "# at each time stamp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XMDzQaeN1tp",
        "outputId": "b4dc1244-3233-45b8-d6fe-d6ca56164025"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00484519 -0.00052032  0.00650519 ... -0.00211109 -0.00122855\n",
            "  -0.00047584]\n",
            " [ 0.00055597  0.00108738  0.00914833 ... -0.00533894 -0.00400132\n",
            "   0.00467944]\n",
            " [ 0.00237425  0.00818407  0.00883197 ... -0.00279903 -0.00172661\n",
            "   0.00293124]\n",
            " ...\n",
            " [ 0.00291738 -0.00068443  0.00918211 ...  0.00668872 -0.00773472\n",
            "   0.00657017]\n",
            " [-0.00445493 -0.00170314  0.01155429 ...  0.00535737 -0.01249553\n",
            "   0.00532854]\n",
            " [-0.00356474 -0.00307771  0.00480465 ...  0.00582648 -0.00690244\n",
            "   0.00658658]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we will look at a prediction at first time step\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# its 65 values representing the probability of each character occuring next."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyUg1jaTPPQZ",
        "outputId": "ec10e8fc-f140-4cb3-d0e3-668fe2a50043"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 4.8451936e-03 -5.2032457e-04  6.5051885e-03 -1.1053171e-03\n",
            " -3.9488324e-03 -5.2038545e-04  3.3307176e-03  1.3175602e-03\n",
            "  6.7410963e-03  2.0613954e-03 -3.7900023e-03 -1.8522888e-04\n",
            " -9.6825641e-03 -1.3918908e-03 -5.7323463e-03 -7.0487303e-03\n",
            " -2.8447923e-03 -1.3721938e-03  7.2481334e-03  1.1895994e-03\n",
            " -3.4496340e-03  4.3425588e-03 -4.0753251e-03 -3.7398853e-03\n",
            " -1.1178544e-03  2.1128780e-03 -4.4179726e-03 -8.6903535e-03\n",
            "  3.1922359e-03  1.2455224e-03 -5.4663426e-04 -1.2829815e-03\n",
            " -3.1931244e-03 -1.7675677e-03 -2.0502363e-03 -1.3676065e-05\n",
            " -1.1985577e-03 -2.9483987e-03 -3.2876036e-04  1.9188905e-03\n",
            " -8.3032288e-03 -2.4958865e-03 -5.0046765e-03 -5.4228930e-03\n",
            "  1.3491050e-03 -9.7656939e-03  1.8994405e-03 -5.2947542e-03\n",
            " -2.3196163e-03  1.9626985e-03  1.2982851e-03  2.7083885e-04\n",
            " -9.7572245e-04 -5.5238120e-03  3.2170513e-04  1.1889951e-03\n",
            "  4.0752236e-03  1.8799721e-03 -2.8376398e-03 -7.5949794e-03\n",
            "  1.8535932e-03  1.5366994e-03 -2.1110945e-03 -1.2285502e-03\n",
            " -4.7583866e-04], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we want to determine predicted character we need \n",
        "# to sample the o/p distribution (pick a value based on probability distribution)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples = 1)\n",
        "\n",
        "# now we can reshape that array and convert all integers to numbers\n",
        "# to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices,(1,-1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "# and this is what our model predicted for training sequence 1\n",
        "predicted_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z1qz3p8LP_Yj",
        "outputId": "85ac6d6a-1a95-4bdf-f88c-306d616ad266"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'JBDTPCxwz&T3?SE:GSdl h\\nZofQm!zck:hfeWltxkMT3WEcFkPokp&MrtI\\ng3!SlBXK PHKPMfoCFN$-tlLg.PakN?\\nvlsH-.ZGY'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will define a loss function\n",
        "# so that it can compare the predicted o/p to the expected o/p\n",
        "# and give us some numeric values representing how close\n",
        "# those two values were\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits, from_logits = True)\n",
        "  "
      ],
      "metadata": {
        "id": "3YxRO5q8RQ06"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling our model\n",
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "metadata": {
        "id": "Cgt9ZU3tScWh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating checkpoints\n",
        "# we are going to setup anf configure our model to save checkpoints as it trains\n",
        "# this will allow us to load our model from a checckpoint and \n",
        "# continue training it.\n",
        "\n",
        "# Directory whr the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True\n",
        ")\n"
      ],
      "metadata": {
        "id": "IU9PIXyOS9Nf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we will start training our model\n",
        "# THE MORE EPOCHS , MORE BETTER OUR MODEL WILL GET IN THIS CASE\n",
        "# ITS NOT GENERALISED BUT IN THIS CASE, MORE EPOCHS =  BETTER MODEL \n",
        "# BECOZ , in these type of models, overfitting is really hard to get\n",
        "# so you can train your model for more epochs to get better results\n",
        "history = model.fit(data, epochs=40,callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJnetDWpUPEb",
        "outputId": "51c74840-4a14-498b-80c9-3097b9ffbcaa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 18s 74ms/step - loss: 2.5871\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.8888\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.6412\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 14s 68ms/step - loss: 1.5074\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.4258\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.3699\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.3243\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 1.2846\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.2483\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.2135\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 14s 68ms/step - loss: 1.1783\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 1.1424\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.1049\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 1.0645\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.0251\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.9832\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.9411\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.9001\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.8600\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 15s 69ms/step - loss: 0.8203\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.7828\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.7488\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.7173\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 15s 70ms/step - loss: 0.6869\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.6602\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 15s 70ms/step - loss: 0.6357\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 15s 71ms/step - loss: 0.6138\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.5944\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.5764\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 16s 72ms/step - loss: 0.5598\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5443\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 17s 69ms/step - loss: 0.5318\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.5196\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.5091\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4999\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 15s 71ms/step - loss: 0.4897\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 15s 71ms/step - loss: 0.4843\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 15s 71ms/step - loss: 0.4749\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 15s 71ms/step - loss: 0.4692\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.4643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING THE MODEL\n",
        "# we will rebuild our mode from a checkpoint using a batch_size = 1\n",
        "# so that we can feed one piece of text to the model and use it to\n",
        "# make a prediction\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size = 1)"
      ],
      "metadata": {
        "id": "yJTjYenUWgHB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once the model is trained\n",
        "# to check the latest checkpoint\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))\n"
      ],
      "metadata": {
        "id": "-2U7FB8iXGFx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can uncomment this, if u want to see any checkpoints\n",
        "# checkpoint_num = 10\n",
        "# model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "# model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "c-ps_6l7XtIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # evaluation step(generating text using the learned model)\n",
        "\n",
        "  # number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers ( vectorizing )\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0 )\n",
        "\n",
        "  text_generated = []\n",
        "  # low temperatures results in more predictable text\n",
        "  # higher temperatures results in more surprising text\n",
        "  # experiment to find the best setting\n",
        "  temperature = 1.0\n",
        "\n",
        "  # here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions,0)\n",
        "    # using a categorical distribution to predict the characters returned by the model\n",
        "    predictions = predictions/temperature\n",
        "    predicted_id = tf.random.categorical(predictions,num_samples = 1)[-1,0].numpy()\n",
        "\n",
        "    # we pass the predicted character as next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id],0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n"
      ],
      "metadata": {
        "id": "3913QwrLYX81"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this code and input any string \n",
        "# our model will generate text based on the romeo - juliet play its being trained on\n",
        "inp = input(\"Type any starting string:  \")\n",
        "print(generate_text(model, inp))\n",
        "# I made myself as a character in this play ,for example , :)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4RRVlVtcwLF",
        "outputId": "c89b89c2-09a2-4d61-b3b7-d45ebc5f2b2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type any starting string:  CHIRAG:\n",
            "CHIRAG:\n",
            "Can your weaken sister ensu,\n",
            "Of holy sun here human,\n",
            "And make your sou more money'd before me\n",
            "And say 'point the mascorror of our heat,\n",
            "And made the monster sungs upon the draw\n",
            "Seeking for her best and so sever it brought a burthen:\n",
            "incestor you, sir, hear me but one drotorney--\n",
            "To keep the oracle where yout in Burk and with humaner, skine on't; let them not soften,\n",
            "Our Romeo hath supply ends: they weep no more suspicing,\n",
            "And send the imagine of it. Thou, Lancaster.\n",
            "\n",
            "KING HENRY VI:\n",
            "Had I but thoughts, alas, whereon theself and there\n",
            "Shall thou be said the tyrant. What!\n",
            "\n",
            "KING RICHARD II:\n",
            "No, to the dignity and swis news is done,\n",
            "And I am content and then ne'er mother;\n",
            "But stir not, rule my life to hold our tents\n",
            "Till he be ceremonious hand,\n",
            "And with your highness told me, I knew not whatso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3wG_kNbdc8bE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}